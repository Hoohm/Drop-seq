{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome Welcome to the documentation of dropSeqPipe v 0.4 . This documentation is still under construction.","title":"Home"},{"location":"#welcome","text":"Welcome to the documentation of dropSeqPipe v 0.4 . This documentation is still under construction.","title":"Welcome"},{"location":"CHANGELOG/","text":"Change Log All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning . [0.5] Added Singularity usage. Try out the --use-singularity option instead of --use-conda Changed Lots off small bugfixes [0.4.1] Added samples.csv and config.yaml schema validation. This will help users fix missing values. DetectBeadSubstitutionErrors was added in the mapping steps. Changed Minimum read length after trimming is now the index of the end of the UMI dropSeqPipe can now run with a docker image if you use the --use-singularity option. This should help people with package issues and different linux setups. You need to have installed singularity system wide to use this option. [0.4] - 2018-12-19 Added Top barcode detection using umi-tools based on number of expected cells. Genome reference and annotation automatically downloaded now base on build and release number from configuration file. On the fly detection of mixed experiment. beta : Generation of a report for publication describing tools used in each steps. run make_report after the preprocessing is done to get reports/publication_text.html . This is a really early stage. Feel free to suggest PR for text modifications. Raw data, results, reference are now independent from the working dir and can be chosen via the configuration file. dropseq_tools v2.0 implemented. This opens up new options such as choosing which locus to use for gene counting. See configuration file. Possibility to edit which biotypes are selected from the annotation file via a gtf_biotypes.yaml file provided. Cell barcodes are now corrected. One hamming distance for known/given whitelists, graphbased correction based on umi-tools for unknown lists. Those corrections are written in the bam files. This makes final bam files compatible for other tools using the XC/XM bam TAGS. UMI are now also corrected based on dropseq_tools v2.0. Possibility to choose SENSE, ANTISENSE or BOTH for read counting. Adapter content for R1 and R2 have now their own plot, adapter_content.pdf . New plot called yield.pdf makes a summary of total reads and how they are distributed among filtered, trimmed, mapped, etc. Configuration file has now a CONTACT section providing a field for a person and a contact e-mail address. Changed Expression matrices output are now sparse (mtx format). This will decrease the size of the output and loading time for downstream analysis. Logfiles, plots and samples output are now grouped together in folders by category. This should make browsing results easier. Fixed most of the packages versions. Summary plots and Seurat object are now in the all rule and will be created by default. Removed Merging of species expression accross samples. Since the mixed experiments are mostly used to test out the doublet rate of a platform and not for downstream analysis, this last part has not been updated. Single expression matrices are still there. Cell barcodes dropped, umi barcodes dropped, starttrim and polyA trim plots are now gone. BC_drop is also removed. Replacements are adapter_content and yield plots. Quality trimming via dropseq_tools has been removed and is now down by cutadapt. Those modifications decrease the running time of the pipeline. [0.32] Added Documentation generated from the markdown files directly on travis-ci. [0.31a] Changed fix on species plot. fix on rule STAR_align adding now unmapped read to a fastq file. Added Added travis integration. The pipeline is now automatically getting tested when updated and when pull requests are proposed. There is now a small git submodule in .test which will provide a sampled file for testing the pipeline on travis-ci. Removed environment.yaml has been removed. Youjust have to install snakemake now instead of activating the env. [0.31] Changed Fixed error for STAR index generation. It crashed saying it couldn't write in folder. Fixed a missing plot for plot_knee_plot_whitelist. Input files for the STAR_align rule have been changed. Adding samples in an already aligned experiment with a different R2 length, will only align the new data and not realign the old one. Split reads and barcodes multiqc reports for qc step. Modified a few rules to follow the guidelines for snakemake workflows Fixed an issue where snakemake would crash on clusters if using expand() on fixed variables such as annotation_prefix . Now using normal python formatting. Changed the config.yaml parameters names to lowercase and hyphens! Software specific variables have their original style making it easier to search in manuals. You will have to either copy the new config.yaml from the templates or modify your own accordingly. cell-barcode-edit-distance changed to what it actually is, UMI-edit-distance. Updated all the envs to fix bugs. Fixed a bug where the mixed species would not run properly. Added Added ggpubr in environment.yaml file. Added a templates folder which will hold config.yaml , samples.csv , cluster.yaml as well as adapters files. This will also help cloning the repository without overwritting your own config.yaml file when updating the pipeline. Added the possibility of using your own adapters fasta file for trimmomatic. To use it, please refer to the WIKI Added fastqc, multiqc, STAR wrappers. You have now to use the --use-conda option to run the pipeline. Added cluster recommendations on the wiki. Added Localrules for certain rules. This allows to run low ressource rules on the host computer instead of nodes when using clusters. genomeChrBinNbits will be calculated automacially for STAR. Exposed all variables for trimmomatic in config.yaml under trimming. Removed png plots have been removed. It was causing some issues on clusters with cairo. Usability is more important than png plots to me. [0.3] Changed Complete overhaul of how the pipeline is organized to follow the structure proposed for snakemake-workflows. This will allow ease of deployement on any platform having conda installed. It will also help to run on clusters. The way to call the pipeline is now simplified. Changes are shown in the WIKI Dependency to Drop-seq-tools updated from version 1.12 to 1.13 Full compatibility with barcode whitelist. Makes it easier to use for SCRBseq protocols or whitelist from other source (UMI-tools). Modified cell and UMI drop plots in order to reflect the option chosen. See plots Removed Bulk sequencing compatiblity. Fastqc and STAR logs plots are removed and replaced by multiqc. Automatic determination of STAMPS via knee_plot. Please use an estimated number of cells as the main threshold and filter in downstream analysis for other parameters such as high number of mitochondrial genes. MinCellFraction entry in config.yaml. This parameter wasn't adding much value and was confusing. Base frequency plot has been removed. This will come back with autodetermination of the STAMPS. Added Wrapper for Drop-seq tools. Makes it easier to switch temp folder and choose maximum memory heap. More parameters for STAR exposed. See WIKI [0.24] Changed All the QCplots are now generated inside the snakefiles. No more generate-plots mode. [0.23a] Changed Will now allow you to run generate-meta without having a config.yaml file in the reference foder. Changed the code for Cell and UMI barcode quality drop (per sample and overall). There was an error in the code not givint the right amount of dropped reads. Updated the images on the wiki accordingly. Fixed the setup where r2py was called before getting installed. Big change in the mapping. From now on the STAR index will be done without a GTF file. This allows to change the overhang option on the fly for each sample based on the mean read length. This also opens up 2-pass mapping. You will have to regenerate your index for it to work. Changed generate_meta in order to fit the new STAR index without a GTF. You now have to give the path to the GTF file in the config.yaml Added min_count_per_umi in the config.yaml to decide how many times a Gene - UMI has to be found to be counted as one. [0.23] Changed pre_align steps will output a fastq.gz instead of a fastq file. fastqc.R is now compatible with paired and single end data. Changed a few options in GLOBAL for UMI and Cell_barcodes options. Now possible to change filtering settings. See WIKI STAR logs have been stripped of the STAR string. This is to allow for better compatibility with multiqc Removed fastqc folder and moved items to logs folder. Grouping all logs files for better multiqc compatibility. Changed generate_meta to generate-meta for keeping similar syntax between modes. Added seperate log files for stats and summary in the DetectBeadSynthesisErrors. Moved part of the README to the wiki. Changed the name of the first expression matrix extracted before the species plot to unfiltered_expression. Added You can now run Bulk Single or paired end RNAseq data. Started a wiki with a FAQ Added options in GLOBAL config.yaml. You can now choose a range of options for UMI and Barcode filtering. please refer to the wiki for more information. Support for MultiQC . MultiQC is a great way of summarising all of the logs from your experiment. As of today it supports 46 different modules (such as fastqc, trimmomatic, STAR, etc...) The generate-plots mode now produces a multiqc_report.html file in the plots folder. New plot! BCDrop.pdf is a new plot showing you how many barcode and UMIs you dropped from the raw data before aligning. This helps to track how many samples you might loose because of low quality reads in the barcoding. [0.22] Changed all subprocess.call replaced by shell from snakemake STAR aligner now not limited to 8 cores or threads but will use the maximum number provided in the local.yaml file Name from dropSeqPip to dropSeqPipe Fixed a bug where all stage1 steps used the same summary file. Now BC tagging, UMI tagging, starting trim and polyA trim have different summary files extract-expression now merges all the samples final count matrix into one per run (folder) Fixed a bug where the amount of total reads on the knee-plot was overinflated. Changed knee-plot mode to generate-plots . Added Temp files have been added in the pipeline. You can turn this off by using the --notemp option fastqc mode now available. Generates fastqc reports plus summary plots Summary file and plot for fastqc and STAR logs Missing R packages should install automatically now. No need to install them beforehand. Report any problem plz GLOBAL values in the config files are now available. They allow to change UMI and BC ranges as well as mismatches for STAR aligner Added a new mode: generate_meta. This allows to create all the metadata files needed for the pipeline. You just need a folder with a genome.fa and an annotation.gtf [0.21] Added Changelog file to track changes --rerun option to force a rerun Multiple steps allowd now [0.2] - 2017-03-14 Changed The pipeline is now a python package being called as an executable Went from json to yaml for config files Added setup.py and dependencies Species plot available Removed primer handling, went to default: AAGCAGTGGTATCAACGCAGAGTAC [0.1] - 2017-02-13 First release Allows for preprocessing, alignement with STAR, post align processing until knee-plot","title":"Change Log"},{"location":"CHANGELOG/#change-log","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning .","title":"Change Log"},{"location":"CHANGELOG/#05","text":"","title":"[0.5]"},{"location":"CHANGELOG/#added","text":"Singularity usage. Try out the --use-singularity option instead of --use-conda","title":"Added"},{"location":"CHANGELOG/#changed","text":"Lots off small bugfixes","title":"Changed"},{"location":"CHANGELOG/#041","text":"","title":"[0.4.1]"},{"location":"CHANGELOG/#added_1","text":"samples.csv and config.yaml schema validation. This will help users fix missing values. DetectBeadSubstitutionErrors was added in the mapping steps.","title":"Added"},{"location":"CHANGELOG/#changed_1","text":"Minimum read length after trimming is now the index of the end of the UMI dropSeqPipe can now run with a docker image if you use the --use-singularity option. This should help people with package issues and different linux setups. You need to have installed singularity system wide to use this option.","title":"Changed"},{"location":"CHANGELOG/#04-2018-12-19","text":"","title":"[0.4] - 2018-12-19"},{"location":"CHANGELOG/#added_2","text":"Top barcode detection using umi-tools based on number of expected cells. Genome reference and annotation automatically downloaded now base on build and release number from configuration file. On the fly detection of mixed experiment. beta : Generation of a report for publication describing tools used in each steps. run make_report after the preprocessing is done to get reports/publication_text.html . This is a really early stage. Feel free to suggest PR for text modifications. Raw data, results, reference are now independent from the working dir and can be chosen via the configuration file. dropseq_tools v2.0 implemented. This opens up new options such as choosing which locus to use for gene counting. See configuration file. Possibility to edit which biotypes are selected from the annotation file via a gtf_biotypes.yaml file provided. Cell barcodes are now corrected. One hamming distance for known/given whitelists, graphbased correction based on umi-tools for unknown lists. Those corrections are written in the bam files. This makes final bam files compatible for other tools using the XC/XM bam TAGS. UMI are now also corrected based on dropseq_tools v2.0. Possibility to choose SENSE, ANTISENSE or BOTH for read counting. Adapter content for R1 and R2 have now their own plot, adapter_content.pdf . New plot called yield.pdf makes a summary of total reads and how they are distributed among filtered, trimmed, mapped, etc. Configuration file has now a CONTACT section providing a field for a person and a contact e-mail address.","title":"Added"},{"location":"CHANGELOG/#changed_2","text":"Expression matrices output are now sparse (mtx format). This will decrease the size of the output and loading time for downstream analysis. Logfiles, plots and samples output are now grouped together in folders by category. This should make browsing results easier. Fixed most of the packages versions. Summary plots and Seurat object are now in the all rule and will be created by default.","title":"Changed"},{"location":"CHANGELOG/#removed","text":"Merging of species expression accross samples. Since the mixed experiments are mostly used to test out the doublet rate of a platform and not for downstream analysis, this last part has not been updated. Single expression matrices are still there. Cell barcodes dropped, umi barcodes dropped, starttrim and polyA trim plots are now gone. BC_drop is also removed. Replacements are adapter_content and yield plots. Quality trimming via dropseq_tools has been removed and is now down by cutadapt. Those modifications decrease the running time of the pipeline.","title":"Removed"},{"location":"CHANGELOG/#032","text":"","title":"[0.32]"},{"location":"CHANGELOG/#added_3","text":"Documentation generated from the markdown files directly on travis-ci.","title":"Added"},{"location":"CHANGELOG/#031a","text":"","title":"[0.31a]"},{"location":"CHANGELOG/#changed_3","text":"fix on species plot. fix on rule STAR_align adding now unmapped read to a fastq file.","title":"Changed"},{"location":"CHANGELOG/#added_4","text":"Added travis integration. The pipeline is now automatically getting tested when updated and when pull requests are proposed. There is now a small git submodule in .test which will provide a sampled file for testing the pipeline on travis-ci.","title":"Added"},{"location":"CHANGELOG/#removed_1","text":"environment.yaml has been removed. Youjust have to install snakemake now instead of activating the env.","title":"Removed"},{"location":"CHANGELOG/#031","text":"","title":"[0.31]"},{"location":"CHANGELOG/#changed_4","text":"Fixed error for STAR index generation. It crashed saying it couldn't write in folder. Fixed a missing plot for plot_knee_plot_whitelist. Input files for the STAR_align rule have been changed. Adding samples in an already aligned experiment with a different R2 length, will only align the new data and not realign the old one. Split reads and barcodes multiqc reports for qc step. Modified a few rules to follow the guidelines for snakemake workflows Fixed an issue where snakemake would crash on clusters if using expand() on fixed variables such as annotation_prefix . Now using normal python formatting. Changed the config.yaml parameters names to lowercase and hyphens! Software specific variables have their original style making it easier to search in manuals. You will have to either copy the new config.yaml from the templates or modify your own accordingly. cell-barcode-edit-distance changed to what it actually is, UMI-edit-distance. Updated all the envs to fix bugs. Fixed a bug where the mixed species would not run properly.","title":"Changed"},{"location":"CHANGELOG/#added_5","text":"Added ggpubr in environment.yaml file. Added a templates folder which will hold config.yaml , samples.csv , cluster.yaml as well as adapters files. This will also help cloning the repository without overwritting your own config.yaml file when updating the pipeline. Added the possibility of using your own adapters fasta file for trimmomatic. To use it, please refer to the WIKI Added fastqc, multiqc, STAR wrappers. You have now to use the --use-conda option to run the pipeline. Added cluster recommendations on the wiki. Added Localrules for certain rules. This allows to run low ressource rules on the host computer instead of nodes when using clusters. genomeChrBinNbits will be calculated automacially for STAR. Exposed all variables for trimmomatic in config.yaml under trimming.","title":"Added"},{"location":"CHANGELOG/#removed_2","text":"png plots have been removed. It was causing some issues on clusters with cairo. Usability is more important than png plots to me.","title":"Removed"},{"location":"CHANGELOG/#03","text":"","title":"[0.3]"},{"location":"CHANGELOG/#changed_5","text":"Complete overhaul of how the pipeline is organized to follow the structure proposed for snakemake-workflows. This will allow ease of deployement on any platform having conda installed. It will also help to run on clusters. The way to call the pipeline is now simplified. Changes are shown in the WIKI Dependency to Drop-seq-tools updated from version 1.12 to 1.13 Full compatibility with barcode whitelist. Makes it easier to use for SCRBseq protocols or whitelist from other source (UMI-tools). Modified cell and UMI drop plots in order to reflect the option chosen. See plots","title":"Changed"},{"location":"CHANGELOG/#removed_3","text":"Bulk sequencing compatiblity. Fastqc and STAR logs plots are removed and replaced by multiqc. Automatic determination of STAMPS via knee_plot. Please use an estimated number of cells as the main threshold and filter in downstream analysis for other parameters such as high number of mitochondrial genes. MinCellFraction entry in config.yaml. This parameter wasn't adding much value and was confusing. Base frequency plot has been removed. This will come back with autodetermination of the STAMPS.","title":"Removed"},{"location":"CHANGELOG/#added_6","text":"Wrapper for Drop-seq tools. Makes it easier to switch temp folder and choose maximum memory heap. More parameters for STAR exposed. See WIKI","title":"Added"},{"location":"CHANGELOG/#024","text":"","title":"[0.24]"},{"location":"CHANGELOG/#changed_6","text":"All the QCplots are now generated inside the snakefiles. No more generate-plots mode.","title":"Changed"},{"location":"CHANGELOG/#023a","text":"","title":"[0.23a]"},{"location":"CHANGELOG/#changed_7","text":"Will now allow you to run generate-meta without having a config.yaml file in the reference foder. Changed the code for Cell and UMI barcode quality drop (per sample and overall). There was an error in the code not givint the right amount of dropped reads. Updated the images on the wiki accordingly. Fixed the setup where r2py was called before getting installed. Big change in the mapping. From now on the STAR index will be done without a GTF file. This allows to change the overhang option on the fly for each sample based on the mean read length. This also opens up 2-pass mapping. You will have to regenerate your index for it to work. Changed generate_meta in order to fit the new STAR index without a GTF. You now have to give the path to the GTF file in the config.yaml","title":"Changed"},{"location":"CHANGELOG/#added_7","text":"min_count_per_umi in the config.yaml to decide how many times a Gene - UMI has to be found to be counted as one.","title":"Added"},{"location":"CHANGELOG/#023","text":"","title":"[0.23]"},{"location":"CHANGELOG/#changed_8","text":"pre_align steps will output a fastq.gz instead of a fastq file. fastqc.R is now compatible with paired and single end data. Changed a few options in GLOBAL for UMI and Cell_barcodes options. Now possible to change filtering settings. See WIKI STAR logs have been stripped of the STAR string. This is to allow for better compatibility with multiqc Removed fastqc folder and moved items to logs folder. Grouping all logs files for better multiqc compatibility. Changed generate_meta to generate-meta for keeping similar syntax between modes. Added seperate log files for stats and summary in the DetectBeadSynthesisErrors. Moved part of the README to the wiki. Changed the name of the first expression matrix extracted before the species plot to unfiltered_expression.","title":"Changed"},{"location":"CHANGELOG/#added_8","text":"You can now run Bulk Single or paired end RNAseq data. Started a wiki with a FAQ Added options in GLOBAL config.yaml. You can now choose a range of options for UMI and Barcode filtering. please refer to the wiki for more information. Support for MultiQC . MultiQC is a great way of summarising all of the logs from your experiment. As of today it supports 46 different modules (such as fastqc, trimmomatic, STAR, etc...) The generate-plots mode now produces a multiqc_report.html file in the plots folder. New plot! BCDrop.pdf is a new plot showing you how many barcode and UMIs you dropped from the raw data before aligning. This helps to track how many samples you might loose because of low quality reads in the barcoding.","title":"Added"},{"location":"CHANGELOG/#022","text":"","title":"[0.22]"},{"location":"CHANGELOG/#changed_9","text":"all subprocess.call replaced by shell from snakemake STAR aligner now not limited to 8 cores or threads but will use the maximum number provided in the local.yaml file Name from dropSeqPip to dropSeqPipe Fixed a bug where all stage1 steps used the same summary file. Now BC tagging, UMI tagging, starting trim and polyA trim have different summary files extract-expression now merges all the samples final count matrix into one per run (folder) Fixed a bug where the amount of total reads on the knee-plot was overinflated. Changed knee-plot mode to generate-plots .","title":"Changed"},{"location":"CHANGELOG/#added_9","text":"Temp files have been added in the pipeline. You can turn this off by using the --notemp option fastqc mode now available. Generates fastqc reports plus summary plots Summary file and plot for fastqc and STAR logs Missing R packages should install automatically now. No need to install them beforehand. Report any problem plz GLOBAL values in the config files are now available. They allow to change UMI and BC ranges as well as mismatches for STAR aligner Added a new mode: generate_meta. This allows to create all the metadata files needed for the pipeline. You just need a folder with a genome.fa and an annotation.gtf","title":"Added"},{"location":"CHANGELOG/#021","text":"","title":"[0.21]"},{"location":"CHANGELOG/#added_10","text":"Changelog file to track changes --rerun option to force a rerun Multiple steps allowd now","title":"Added"},{"location":"CHANGELOG/#02-2017-03-14","text":"","title":"[0.2] - 2017-03-14"},{"location":"CHANGELOG/#changed_10","text":"The pipeline is now a python package being called as an executable Went from json to yaml for config files","title":"Changed"},{"location":"CHANGELOG/#added_11","text":"setup.py and dependencies Species plot available","title":"Added"},{"location":"CHANGELOG/#removed_4","text":"primer handling, went to default: AAGCAGTGGTATCAACGCAGAGTAC","title":"Removed"},{"location":"CHANGELOG/#01-2017-02-13","text":"","title":"[0.1] - 2017-02-13"},{"location":"CHANGELOG/#first-release","text":"Allows for preprocessing, alignement with STAR, post align processing until knee-plot","title":"First release"},{"location":"Clusters/","text":"Running on clusters There is a file in the templates called cluster.yaml . This can be used to modify ressources needed for your data. I generally recommand moving the file to the root of the folder so that it doesn't get replaced by updates. Bellow is an example of running on a cluster using the template file cluster.yaml on SLURM. snakemake --cluster 'sbatch -n {cluster.n} -t {cluster.time} --clusters=CLUSTERNAME --output={cluster.output}' --jobs N --cluster-config cluster.yaml --use-conda --local-cores C N: is the number of jobs you are allowed to run at the same time C: is the local-cores of the host machine. A few simple rules are gonna be run locally (not sent to nodes) because they are not that heavy (mostly plotting) CLUSTERNAME: the name of the cluster you want to use Note: The default path for cluster logs in the cluster.yaml is logs/cluster/ . If that folder doesn't exist, our cluster can't write and will crash without an error message.","title":"Clusters"},{"location":"Clusters/#running-on-clusters","text":"There is a file in the templates called cluster.yaml . This can be used to modify ressources needed for your data. I generally recommand moving the file to the root of the folder so that it doesn't get replaced by updates. Bellow is an example of running on a cluster using the template file cluster.yaml on SLURM. snakemake --cluster 'sbatch -n {cluster.n} -t {cluster.time} --clusters=CLUSTERNAME --output={cluster.output}' --jobs N --cluster-config cluster.yaml --use-conda --local-cores C N: is the number of jobs you are allowed to run at the same time C: is the local-cores of the host machine. A few simple rules are gonna be run locally (not sent to nodes) because they are not that heavy (mostly plotting) CLUSTERNAME: the name of the cluster you want to use Note: The default path for cluster logs in the cluster.yaml is logs/cluster/ . If that folder doesn't exist, our cluster can't write and will crash without an error message.","title":"Running on clusters"},{"location":"Create-config-files/","text":"Config file and sample file In order to run the pipeline you will need to complete the config.yaml file and the samples.csv file. Both are located in the templates folder , should be moved to the root folder of the experiment and filled in for missing entries before running the pipeline. The goal for this is to provide the config.yaml when you finally upload the data to a repository for a publication as well as the pipeline version. This provides other users to ability to rerun the processing from scratch exactly as you did. This is possible because snakemake will download and create the exact same environnment for each rule using the envs files provided with the pipeline. 1. config.yaml - Executables, system and experiment parameters The config.yaml contains all the necessary parameters and paths for the pipeline. CONTACT: email: user.name@provider.com person: John Doe LOCAL: temp-directory: /tmp memory: 4g raw_data: results: META: species: mus_musculus: build: 38 release: 94 homo_sapiens: build: 38 release: 91 ratio: 0.2 reference-directory: /path/to/references/ gtf_biotypes: gtf_biotypes.yaml FILTER: barcode_whitelist: '' 5-prime-smart-adapter: AAAAAAAAAAA cell-barcode: start: 1 end: 12 UMI-barcode: start: 13 end: 20 cutadapt: adapters-file: 'adapters.fa' R1: quality-filter: 20 maximum-Ns: 0 extra-params: '' R2: quality-filter: 20 minimum-adapters-overlap: 6 minimum-length: 15 extra-params: '' MAPPING: STAR: genomeChrBinNbits: 18 outFilterMismatchNmax: 10 outFilterMismatchNoverLmax: 0.3 outFilterMismatchNoverReadLmax: 1 outFilterMatchNmin: 0 outFilterMatchNminOverLread: 0.66 outFilterScoreMinOverLread: 0.66 EXTRACTION: LOCUS: - CODING - UTR strand-strategy: SENSE UMI-edit-distance: 1 minimum-counts-per-UMI: 0 Please note the \"space\" after the colon, is needed for the yaml to work. Subsections [CONTACT] email and person This is not requested. You can provide the e-mail and name address of the person who processed the data using this configuration. Ideally you should provide the config.yaml with the data repository to allow people to rerun the data using dropSeqPipe. [LOCAL] temp-directory is the temp or scratch folder with enough space to keep temporary files. memory is the maximum memory allocation pool for a Java Virtual Machine. raw_data is the folder containing all your raw fastq.gz files. results is the folder that will contain all the results of the pipeline. [META] species is where you list the species of your samples. It can be a mixed experiment with two entries. SPECIES_ONE can be for example: mus_musculus, homo_sapiens, etc... It has to be the name used on ensembl for automatic download to work. build is the genome build number. release is the annotation release number. SPECIES_TWO can be your second species. ratio is how much \"contamination\" from another species you allow to validate them as a species or mixed. 0.2 means you allow a maximum of 20% mixing. reference-directory is where you want to store your references files. gtf_biotypes is the gtf_biotypes.yaml file containing the selection of biotypes you want to keep for your gene to read attribution. Using less biotypes may decrease your multimapping counts. [FILTER] barcode_whitelist is the filename of your whitelist fi you have one. Well plate base protocols often have one. 5-prime-smart-adapter is the 5\" smart adapter used in your protocol. cell-barcode and UMI-barcode : Is the section for cell/umi barcode filtering. start is the first base position of your cell/umi barcode. end is the last base position of your cell/umi barcode. cutadapt : Is the section for trimming. adapters-file is the file containing your list of adapters as fasta. you can choose between 6 files in the templates folder, add any sequence to existing files or provide your own custom one. NexteraPE-PE.fa TruSeq2-PE.fa TruSeq2-SE.fa TruSeq3-PE-2.fa TruSeq3-PE.fa TruSeq3-SE.fa Provide the path to the file you want to use for trimming. If you want to add custom sequences or create a complete new one, I would advise to store it in the ROOT folder of the experiment. This will ensure that your custom file will not be overwritten if you update the pipeline. Example: NexteraPE-PE.fa R1 lists the options for read1 (cell barcode and umi) filtering/trimming * quality-filter is the minimum mean score of the sliding window for quality filtering. * maximum-Ns how many Ns you allow in the cell barcode and umi barcode. By default it is one because we want to be able to collapse barcodes that have one mismatch. * extra-params if you usually add extra paramters to cutadapt, you can do it here. Only for experienced cutadapt users . R2 lists the options for read2 (mRNA) filtering/trimming that have one mismatch. * maximum-length is the maximum length of your mRNA read before alignement. * extra-params if you usually add extra paramters to cutadapt, you can do it here. Only for experienced cutadapt users . For more information about trimming and filtering please visit the cutadapt website. [MAPPING] STAR genomeChrBinNbits is a value used for index generation in STAR. The formula is min(18,int(log2(genomeLength/referenceNumber))) outFilterMismatchNmax (default:10) is the maximum number of mismatches allowed. outFilterMismatchNoverLmax (default:0.3) is the maximum ratio of mismatched bases that mapped. outFilterMismatchNoverReadLmax (default:1.0) is the maximum ratio of mismatched bases of the whole read. outFilterMatchNmin (default:0) is the minimum number of matched bases. outFilterMatchNminOverLread (default:0.66) alignment will be output only if the ratio of matched bases is higher than or equal to this value. outFilterScoreMinOverLread (default:0.66) alignment will be output only if its ratio score is higher than or equal to this value. All of the values for STAR are the default ones. For details about STAR parameters and what they do, please refer to the STAR manual on git . [EXTRACTION] LOCUS are the overlapping regions that reads overlap and are counted in the final expression matrix. Possible values are CODING , UTR , INTRON UMI-edit-distance This is the maximum manhattan distance between two UMI barcode when extracting count matrices. min-count-per-umi is the minimum UMI/Gene pair needed to be counted as one. strand-strategy SENSE defines that you only count genes where the forward strand mapped to the forward region on the DNA. Other possibilities are ANTISENSE (only count reads that mapped on the opposite strand) or BOTH (count all). 2. samples.csv - Samples parameters This file holds the sample names, expected cell numbers and read length for each sample. The file has to have this format: samples,expected_cells,read_lengths,batch sample_name1,500,100,Batch1 sample_name2,500,100,Batch2 expected_cells is the amount of cells you expect from your sample. read_length is the read length of the mRNA (Read2). This is necessary for STAR index generation batch is the batch of your sample. If you are added new samples to the same experiment, this is typically a good place to add the main batch. Note: You can add any other column you wish here, it won't affect the pipeline and you can use it later on in your analysis. Finally, you can now run the pipeline or Create a custom reference","title":"Config file and sample file"},{"location":"Create-config-files/#config-file-and-sample-file","text":"In order to run the pipeline you will need to complete the config.yaml file and the samples.csv file. Both are located in the templates folder , should be moved to the root folder of the experiment and filled in for missing entries before running the pipeline. The goal for this is to provide the config.yaml when you finally upload the data to a repository for a publication as well as the pipeline version. This provides other users to ability to rerun the processing from scratch exactly as you did. This is possible because snakemake will download and create the exact same environnment for each rule using the envs files provided with the pipeline.","title":"Config file and sample file"},{"location":"Create-config-files/#1-configyaml-executables-system-and-experiment-parameters","text":"The config.yaml contains all the necessary parameters and paths for the pipeline. CONTACT: email: user.name@provider.com person: John Doe LOCAL: temp-directory: /tmp memory: 4g raw_data: results: META: species: mus_musculus: build: 38 release: 94 homo_sapiens: build: 38 release: 91 ratio: 0.2 reference-directory: /path/to/references/ gtf_biotypes: gtf_biotypes.yaml FILTER: barcode_whitelist: '' 5-prime-smart-adapter: AAAAAAAAAAA cell-barcode: start: 1 end: 12 UMI-barcode: start: 13 end: 20 cutadapt: adapters-file: 'adapters.fa' R1: quality-filter: 20 maximum-Ns: 0 extra-params: '' R2: quality-filter: 20 minimum-adapters-overlap: 6 minimum-length: 15 extra-params: '' MAPPING: STAR: genomeChrBinNbits: 18 outFilterMismatchNmax: 10 outFilterMismatchNoverLmax: 0.3 outFilterMismatchNoverReadLmax: 1 outFilterMatchNmin: 0 outFilterMatchNminOverLread: 0.66 outFilterScoreMinOverLread: 0.66 EXTRACTION: LOCUS: - CODING - UTR strand-strategy: SENSE UMI-edit-distance: 1 minimum-counts-per-UMI: 0 Please note the \"space\" after the colon, is needed for the yaml to work.","title":"1. config.yaml - Executables, system and experiment parameters"},{"location":"Create-config-files/#subsections","text":"","title":"Subsections"},{"location":"Create-config-files/#contact","text":"email and person This is not requested. You can provide the e-mail and name address of the person who processed the data using this configuration. Ideally you should provide the config.yaml with the data repository to allow people to rerun the data using dropSeqPipe.","title":"[CONTACT]"},{"location":"Create-config-files/#local","text":"temp-directory is the temp or scratch folder with enough space to keep temporary files. memory is the maximum memory allocation pool for a Java Virtual Machine. raw_data is the folder containing all your raw fastq.gz files. results is the folder that will contain all the results of the pipeline.","title":"[LOCAL]"},{"location":"Create-config-files/#meta","text":"species is where you list the species of your samples. It can be a mixed experiment with two entries. SPECIES_ONE can be for example: mus_musculus, homo_sapiens, etc... It has to be the name used on ensembl for automatic download to work. build is the genome build number. release is the annotation release number. SPECIES_TWO can be your second species. ratio is how much \"contamination\" from another species you allow to validate them as a species or mixed. 0.2 means you allow a maximum of 20% mixing. reference-directory is where you want to store your references files. gtf_biotypes is the gtf_biotypes.yaml file containing the selection of biotypes you want to keep for your gene to read attribution. Using less biotypes may decrease your multimapping counts.","title":"[META]"},{"location":"Create-config-files/#filter","text":"barcode_whitelist is the filename of your whitelist fi you have one. Well plate base protocols often have one. 5-prime-smart-adapter is the 5\" smart adapter used in your protocol. cell-barcode and UMI-barcode : Is the section for cell/umi barcode filtering. start is the first base position of your cell/umi barcode. end is the last base position of your cell/umi barcode. cutadapt : Is the section for trimming. adapters-file is the file containing your list of adapters as fasta. you can choose between 6 files in the templates folder, add any sequence to existing files or provide your own custom one. NexteraPE-PE.fa TruSeq2-PE.fa TruSeq2-SE.fa TruSeq3-PE-2.fa TruSeq3-PE.fa TruSeq3-SE.fa Provide the path to the file you want to use for trimming. If you want to add custom sequences or create a complete new one, I would advise to store it in the ROOT folder of the experiment. This will ensure that your custom file will not be overwritten if you update the pipeline. Example: NexteraPE-PE.fa R1 lists the options for read1 (cell barcode and umi) filtering/trimming * quality-filter is the minimum mean score of the sliding window for quality filtering. * maximum-Ns how many Ns you allow in the cell barcode and umi barcode. By default it is one because we want to be able to collapse barcodes that have one mismatch. * extra-params if you usually add extra paramters to cutadapt, you can do it here. Only for experienced cutadapt users . R2 lists the options for read2 (mRNA) filtering/trimming that have one mismatch. * maximum-length is the maximum length of your mRNA read before alignement. * extra-params if you usually add extra paramters to cutadapt, you can do it here. Only for experienced cutadapt users . For more information about trimming and filtering please visit the cutadapt website.","title":"[FILTER]"},{"location":"Create-config-files/#mapping","text":"STAR genomeChrBinNbits is a value used for index generation in STAR. The formula is min(18,int(log2(genomeLength/referenceNumber))) outFilterMismatchNmax (default:10) is the maximum number of mismatches allowed. outFilterMismatchNoverLmax (default:0.3) is the maximum ratio of mismatched bases that mapped. outFilterMismatchNoverReadLmax (default:1.0) is the maximum ratio of mismatched bases of the whole read. outFilterMatchNmin (default:0) is the minimum number of matched bases. outFilterMatchNminOverLread (default:0.66) alignment will be output only if the ratio of matched bases is higher than or equal to this value. outFilterScoreMinOverLread (default:0.66) alignment will be output only if its ratio score is higher than or equal to this value. All of the values for STAR are the default ones. For details about STAR parameters and what they do, please refer to the STAR manual on git .","title":"[MAPPING]"},{"location":"Create-config-files/#extraction","text":"LOCUS are the overlapping regions that reads overlap and are counted in the final expression matrix. Possible values are CODING , UTR , INTRON UMI-edit-distance This is the maximum manhattan distance between two UMI barcode when extracting count matrices. min-count-per-umi is the minimum UMI/Gene pair needed to be counted as one. strand-strategy SENSE defines that you only count genes where the forward strand mapped to the forward region on the DNA. Other possibilities are ANTISENSE (only count reads that mapped on the opposite strand) or BOTH (count all).","title":"[EXTRACTION]"},{"location":"Create-config-files/#2-samplescsv-samples-parameters","text":"This file holds the sample names, expected cell numbers and read length for each sample. The file has to have this format: samples,expected_cells,read_lengths,batch sample_name1,500,100,Batch1 sample_name2,500,100,Batch2 expected_cells is the amount of cells you expect from your sample. read_length is the read length of the mRNA (Read2). This is necessary for STAR index generation batch is the batch of your sample. If you are added new samples to the same experiment, this is typically a good place to add the main batch. Note: You can add any other column you wish here, it won't affect the pipeline and you can use it later on in your analysis. Finally, you can now run the pipeline or Create a custom reference","title":"2. samples.csv - Samples parameters"},{"location":"FAQ/","text":"FAQ 1. I get error='Cannot allocate memory' (errno=12) , what should I do. [Fixed] This has been fixed by using a wrapper exposing the TMPDIR to the pipeline. First, be sure that your TMPDIR from the first configuration yaml has at least 100Go. If you still have problems, you should edit the following files in the Drop-seq_tools-1.12: TagBamWithReadSequenceExtended FilterBAM TrimStartingSequence PolyATrimmer TagReadWithGeneExon DetectBeadSynthesisErrors SingleCellRnaSeqMetricsCollector BAMTagHistogram In each of those files, the last line should be something like: java -Xmx${xmx} -Djava.io.tmpdir=/path/to/temp/folder/ -jar $jar_deploy_dir/dropseq.jar $progname $* You can also use this simple bash script to do it: Replace /path/to/temp/folder/ with your temp path and don't forget to use escapes for / for f in BAMTagHistogram SingleCellRnaSeqMetricsCollector DetectBeadSynthesisErrors TagReadWithGeneExon PolyATrimmer TrimStartingSequence FilterBAM TagBamWithReadSequenceExtended do sed -i 's/java -Xmx${xmx}/java -Xmx${xmx} -Djava.io.tmpdir=/path/to/temp/folder/ /g' $f done","title":"FAQ"},{"location":"FAQ/#faq","text":"","title":"FAQ"},{"location":"FAQ/#1-i-get-errorcannot-allocate-memory-errno12-what-should-i-do-fixed","text":"This has been fixed by using a wrapper exposing the TMPDIR to the pipeline. First, be sure that your TMPDIR from the first configuration yaml has at least 100Go. If you still have problems, you should edit the following files in the Drop-seq_tools-1.12: TagBamWithReadSequenceExtended FilterBAM TrimStartingSequence PolyATrimmer TagReadWithGeneExon DetectBeadSynthesisErrors SingleCellRnaSeqMetricsCollector BAMTagHistogram In each of those files, the last line should be something like: java -Xmx${xmx} -Djava.io.tmpdir=/path/to/temp/folder/ -jar $jar_deploy_dir/dropseq.jar $progname $* You can also use this simple bash script to do it: Replace /path/to/temp/folder/ with your temp path and don't forget to use escapes for / for f in BAMTagHistogram SingleCellRnaSeqMetricsCollector DetectBeadSynthesisErrors TagReadWithGeneExon PolyATrimmer TrimStartingSequence FilterBAM TagBamWithReadSequenceExtended do sed -i 's/java -Xmx${xmx}/java -Xmx${xmx} -Djava.io.tmpdir=/path/to/temp/folder/ /g' $f done","title":"1. I get error='Cannot allocate memory' (errno=12), what should I do. [Fixed]"},{"location":"Installation/","text":"This pipeline is dependent on conda. Step 1: Download and install miniconda3 First you need to download and install miniconda3: for linux wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh for mac os curl https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -o Miniconda3-latest-MacOSX-x86_64.sh bash Miniconda3-latest-MacOSX-x86_64.sh Step 2: Clone the workflow Clone the worflow git clone https://github.com/Hoohm/dropSeqPipe.git Step 3: Install snakemake conda install -c bioconda -c conda-forge snakemake Next step is config files completion Complete the config.yaml with the missing information UPDATES: How to update the pipeline Go to your experiment folder, then pull. git pull https://github.com/Hoohm/dropSeqPipe.git If you want to update files/plots based on the updates you can use this command: snakemake -R `snakemake --list-codes-changes` This will update all the files that would be modified by the changes in the code (rules or script). Depending on how much and where the changes have been made, this might rerun the whole pipeline.","title":"Installation"},{"location":"Installation/#step-1-download-and-install-miniconda3","text":"First you need to download and install miniconda3: for linux wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh for mac os curl https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -o Miniconda3-latest-MacOSX-x86_64.sh bash Miniconda3-latest-MacOSX-x86_64.sh","title":"Step 1: Download and install miniconda3"},{"location":"Installation/#step-2-clone-the-workflow","text":"Clone the worflow git clone https://github.com/Hoohm/dropSeqPipe.git","title":"Step 2: Clone the workflow"},{"location":"Installation/#step-3-install-snakemake","text":"conda install -c bioconda -c conda-forge snakemake Next step is config files completion Complete the config.yaml with the missing information","title":"Step 3: Install snakemake"},{"location":"Installation/#updates-how-to-update-the-pipeline","text":"Go to your experiment folder, then pull. git pull https://github.com/Hoohm/dropSeqPipe.git If you want to update files/plots based on the updates you can use this command: snakemake -R `snakemake --list-codes-changes` This will update all the files that would be modified by the changes in the code (rules or script). Depending on how much and where the changes have been made, this might rerun the whole pipeline.","title":"UPDATES: How to update the pipeline"},{"location":"Plots/","text":"On of the main purpose of this package is getting information about your data to improve your protocol and filter your data for further downstream analysis. Here is a list of plots and reports that you will get from the pipeline. Fastqc, STAR and cutadapt reports are generated as multiqc reports in the reports folder. 1. Adapter content On the x axis are the samples. On the y axis are the percentages of total adapters that have been found (and trimmed) in respective fastq files based on the adapter-file provided via config.yaml . The top plot is for read1 and the bottom for read2. This plot provides an idea of the which adapter has been found and in which proportion in each sample. 2. Yield (across samples) On the x axis are the samples. TOP: On the y axis are the number of reads attributed to each category. BOTTOM: On the y axis are the percentage of attributed to each category. This plot gives you an overview of all the reads from your samples and how they are distributed in all the possible categories. The reads that are uniquely mapped ar the ones you will keep at the end for the UMI count matrix. 3. Knee plot (per sample) On the x axis is the cumulative fraction of reads per STAMPS (captured cell). On the y axis is the ordered STAMPS (based on total reads). This allows you to determine how much of the reads you actually captured with the number of cells you expected. The cutting is based on the expected_cells parameter in the samples.csv file. The green selected cells are the cells that are going to be in the final expression matrix. If you see a clear bend on the plot that is higher in the number of cells than what you expected, you should increase the expected_cells value and rerun the extract step. If it is under, I would advise to filter out your data with a downstream analysis tool such as Seurat. Note: I advise not to try to discover \"real\" cells/STAMPS at this stage. I suggest to extract the expected number of cells and filter out later in post-processing with other kind of meta data. 4. RNA metrics (per sample) On the x axis are top barcodes based on your expected_cells values or the barcodes.csv file. Top plot: On the y axis are the number of bases classified by region of mapping. Bottom plot: On the y axis are the percentage of bases classified by region of mapping. This plot gives a lot of different informations. The top plot allows you to quickly compare cells between them in terms of how much has been mapped. This can sometimes help identify outliers or bad runs. The bottom plot allows you to find cells that have an \"abnormal\" mapped base distribution compared to other cells. 5. Violine plots for barcode properties (across samples) Various statistic for barcodes that were taken forward as STAMPs as set as expected_cells in config.yaml . Each point represents a barcode augmented by a violine-plot density estimator of barcode distribution along the y-axis. On the x axis are the samples for each panel (Note: the dot distribution along the x-axis does't not bear information, it's just a visual aid to better assess density). On the y axis are the respecitve statistics described below for each panel. TOP panel from left to right: nUMI: number of UMI per barcode nCounts: number of Counts per barcode top50: fraction (percentage/100) of the highest expressed genes compared to entire set of genes. BOTTOM: nUMI: average number of UMI per Gene per barcode pct.Ribo: Fraction of ribosomal RNA (Note: ribsomal transcripts defined as starting with \"^Rpl\") pct.mito: Fraction of mitochondrial RNA (Note: mitchondrial transcripts defined as starting with \"^mt-\") 6. Saturation plot: UMI per barcode (across samples) Number of UMI (x-axis) vs number of Genes (y-axis) for each barcode (points in plot) broken down by sample (different colors). Number of Genes defined as Genes having at least 1 read mapped to them. Individual samples are color-coded. A loess regression curve of barcodes for each sample is fitted. Various statistic for barcodes that were taken forward as STAMPs as set as expected_cells in config.yaml . This plot can indicate how many counts per barcode are required on average to find all expressed genes in a cell. Given enought coverage, it can also indicate how many genes are expressed for the examined cell type. 7. Saturation plot: Counts per barcode (across samples) Number of Counts (x-axis) vs number of Genes (y-axis) for each barcode (points in plot) broken down by sample (different colors). Number of Genes defined as Genes having at least 1 read mapped to them. Individual samples are color-coded. A loess regression curve of barcodes for each sample is fitted. Various statistic for barcodes that were taken forward as STAMPs as set as expected_cells in config.yaml . 8. Counts per UMI per barcode (across samples) Number of UMI (x-axis) vs number of Counts (y-axis) for each barcode (points in plot) broken down by sample (different colors). Individual samples are color-coded. A loess regression curve of barcodes for each sample is fitted. Black line indicate an optimal 1:1 ratio between UMI and Counts (i.e. no Duplicates!) This plots can give an indication on the level of duplication for each sample. The close to black line the lower duplication. Mixed experiment 9. Barnyard plot (per sample) This plot shows you species purity for each STAMPS. Mixed and No call STAMPS are dropped and only single species are kept for extraction. You can change the minimum ratio of transcripts to define a STAMP as mixed or not in the configfile with: species_ratio You get one plot for genes and one plot for transcripts. The selection is done on the transcript level.","title":"Plots"},{"location":"Plots/#1-adapter-content","text":"On the x axis are the samples. On the y axis are the percentages of total adapters that have been found (and trimmed) in respective fastq files based on the adapter-file provided via config.yaml . The top plot is for read1 and the bottom for read2. This plot provides an idea of the which adapter has been found and in which proportion in each sample.","title":"1. Adapter content"},{"location":"Plots/#2-yield-across-samples","text":"On the x axis are the samples. TOP: On the y axis are the number of reads attributed to each category. BOTTOM: On the y axis are the percentage of attributed to each category. This plot gives you an overview of all the reads from your samples and how they are distributed in all the possible categories. The reads that are uniquely mapped ar the ones you will keep at the end for the UMI count matrix.","title":"2. Yield (across samples)"},{"location":"Plots/#3-knee-plot-per-sample","text":"On the x axis is the cumulative fraction of reads per STAMPS (captured cell). On the y axis is the ordered STAMPS (based on total reads). This allows you to determine how much of the reads you actually captured with the number of cells you expected. The cutting is based on the expected_cells parameter in the samples.csv file. The green selected cells are the cells that are going to be in the final expression matrix. If you see a clear bend on the plot that is higher in the number of cells than what you expected, you should increase the expected_cells value and rerun the extract step. If it is under, I would advise to filter out your data with a downstream analysis tool such as Seurat. Note: I advise not to try to discover \"real\" cells/STAMPS at this stage. I suggest to extract the expected number of cells and filter out later in post-processing with other kind of meta data.","title":"3. Knee plot (per sample)"},{"location":"Plots/#4-rna-metrics-per-sample","text":"On the x axis are top barcodes based on your expected_cells values or the barcodes.csv file. Top plot: On the y axis are the number of bases classified by region of mapping. Bottom plot: On the y axis are the percentage of bases classified by region of mapping. This plot gives a lot of different informations. The top plot allows you to quickly compare cells between them in terms of how much has been mapped. This can sometimes help identify outliers or bad runs. The bottom plot allows you to find cells that have an \"abnormal\" mapped base distribution compared to other cells.","title":"4. RNA metrics (per sample)"},{"location":"Plots/#5-violine-plots-for-barcode-properties-across-samples","text":"Various statistic for barcodes that were taken forward as STAMPs as set as expected_cells in config.yaml . Each point represents a barcode augmented by a violine-plot density estimator of barcode distribution along the y-axis. On the x axis are the samples for each panel (Note: the dot distribution along the x-axis does't not bear information, it's just a visual aid to better assess density). On the y axis are the respecitve statistics described below for each panel. TOP panel from left to right: nUMI: number of UMI per barcode nCounts: number of Counts per barcode top50: fraction (percentage/100) of the highest expressed genes compared to entire set of genes. BOTTOM: nUMI: average number of UMI per Gene per barcode pct.Ribo: Fraction of ribosomal RNA (Note: ribsomal transcripts defined as starting with \"^Rpl\") pct.mito: Fraction of mitochondrial RNA (Note: mitchondrial transcripts defined as starting with \"^mt-\")","title":"5. Violine plots for barcode properties (across samples)"},{"location":"Plots/#6-saturation-plot-umi-per-barcode-across-samples","text":"Number of UMI (x-axis) vs number of Genes (y-axis) for each barcode (points in plot) broken down by sample (different colors). Number of Genes defined as Genes having at least 1 read mapped to them. Individual samples are color-coded. A loess regression curve of barcodes for each sample is fitted. Various statistic for barcodes that were taken forward as STAMPs as set as expected_cells in config.yaml . This plot can indicate how many counts per barcode are required on average to find all expressed genes in a cell. Given enought coverage, it can also indicate how many genes are expressed for the examined cell type.","title":"6. Saturation plot: UMI per barcode (across samples)"},{"location":"Plots/#7-saturation-plot-counts-per-barcode-across-samples","text":"Number of Counts (x-axis) vs number of Genes (y-axis) for each barcode (points in plot) broken down by sample (different colors). Number of Genes defined as Genes having at least 1 read mapped to them. Individual samples are color-coded. A loess regression curve of barcodes for each sample is fitted. Various statistic for barcodes that were taken forward as STAMPs as set as expected_cells in config.yaml .","title":"7. Saturation plot: Counts per barcode (across samples)"},{"location":"Plots/#8-counts-per-umi-per-barcode-across-samples","text":"Number of UMI (x-axis) vs number of Counts (y-axis) for each barcode (points in plot) broken down by sample (different colors). Individual samples are color-coded. A loess regression curve of barcodes for each sample is fitted. Black line indicate an optimal 1:1 ratio between UMI and Counts (i.e. no Duplicates!) This plots can give an indication on the level of duplication for each sample. The close to black line the lower duplication.","title":"8. Counts per UMI per barcode (across samples)"},{"location":"Plots/#mixed-experiment","text":"","title":"Mixed experiment"},{"location":"Plots/#9-barnyard-plot-per-sample","text":"This plot shows you species purity for each STAMPS. Mixed and No call STAMPS are dropped and only single species are kept for extraction. You can change the minimum ratio of transcripts to define a STAMP as mixed or not in the configfile with: species_ratio You get one plot for genes and one plot for transcripts. The selection is done on the transcript level.","title":"9. Barnyard plot (per sample)"},{"location":"Reference-Files/","text":"Reference files From version 0.4 on, reference files are automatically downloaded by the pipeline. Mixed references are also downloaded and merged automatically. Since sometimes you still want to use your own reference you can bypass the download by creating your own genome.fa and annotation.gtf file. Snakemake generates file based on paths. If you want to use a custom reference you have to name it properly for snakemake to find it. Here is an example: Let's assume this is you configuration for the META section: META: species: funky_species_name: build: A release: 1 ratio: 0.2 reference-directory: /absolute/path/to/references gtf_biotypes: gtf_biotypes.yaml You need to provide the following files /absolute/path/to/references/funky_species_name_A_1/genome.fa /absolute/path/to/references/funky_species_name_A_1/annotation.gtf This will stop dropSeqPipe from downloading a new reference. Once the pipeline has run completely, the folder will look like this: genome.fa annotation.gtf annotation.refFlat annotation_reduced.gtf genome.consensus_introns.intervals genome.dict genome.exons.intervals genome.genes.intervals genome.intergenic.intervals genome.rRNA.intervals STAR_INDEX/SA_read_length/ Note: The STAR index will be built based on the read length of your mRNA read (Read2). If you have different lengths, it will produce multiple indexes. Finally, you can now run the pipeline","title":"Reference Files"},{"location":"Reference-Files/#reference-files","text":"From version 0.4 on, reference files are automatically downloaded by the pipeline. Mixed references are also downloaded and merged automatically. Since sometimes you still want to use your own reference you can bypass the download by creating your own genome.fa and annotation.gtf file. Snakemake generates file based on paths. If you want to use a custom reference you have to name it properly for snakemake to find it. Here is an example: Let's assume this is you configuration for the META section: META: species: funky_species_name: build: A release: 1 ratio: 0.2 reference-directory: /absolute/path/to/references gtf_biotypes: gtf_biotypes.yaml You need to provide the following files /absolute/path/to/references/funky_species_name_A_1/genome.fa /absolute/path/to/references/funky_species_name_A_1/annotation.gtf This will stop dropSeqPipe from downloading a new reference. Once the pipeline has run completely, the folder will look like this: genome.fa annotation.gtf annotation.refFlat annotation_reduced.gtf genome.consensus_introns.intervals genome.dict genome.exons.intervals genome.genes.intervals genome.intergenic.intervals genome.rRNA.intervals STAR_INDEX/SA_read_length/ Note: The STAR index will be built based on the read length of your mRNA read (Read2). If you have different lengths, it will produce multiple indexes. Finally, you can now run the pipeline","title":"Reference files"},{"location":"Running-dropSeqPipe/","text":"Example The pipeline is to be cloned once and then run on any folder containing the configuration files and your raw data. The workingdir folder can contain multiple runs (aka batches) as you can easily add new samples when recieving new data and run the same commands. This will simply run the pipeline on the newly added data and recreate reports as well as plots containing all the samples. Example: You run 2 biological conditions with 2 replicates. This makes up for 4 samples. Assume a simple dropseq protocol with only human cells. 1. You sequence the data and recieve the 8 files (two files per sample) and download the pipeline 2. You run the pipeline with the command: snakemake --use-conda --cores N --directory WORKING_DIR . N being the number of cores available and WORKING_DIR being the folder containing your config.yaml , samples.csv , adapter file and gtf_biotypes.yaml . 3. You see that there is an issue with the protocol and you modify it 4. You create a new set of libraries and sequence them (same 2x2 design) 5. You add the new files in the data folder of WORKING_DIR and edit the samples.csv to add missing samples. 6. You run the pipeline as you did the first time snakemake --use-conda --cores N --directory WORKING_DIR 7. This will run the new samples only and recreate the reports as well as the yield plots. 8. It is now easy to compare the impact of your change in the procotol Working dir folder preparation The raw data from the sequencer should be stored in the RAW_DATA folder of WORKING_DIR folder like this: /path/to/your/WORKING_DIR/ | -- RAW_DATA/ | -- -- sample1_R1.fastq.gz | -- -- sample1_R2.fastq.gz | -- -- sample2_R1.fastq.gz | -- -- sample2_R2.fastq.gz | samples.csv | config.yaml | barcodes.csv | adapters.fa Note: In DropSeq or ScrbSeq you expect a paired sequencing. R1 will hold the information of your barcode and UMI, R2 will hold the 3' end of the captured mRNA. Once everything is in place, you can run the pipeline using the normal snakemake commands. Running the pipeline (TLDR version) For a simple single cell run you only need to run: snakemake --cores N --use-conda --directory WORKING_DIR This will run the whole pipeline and use the X number of cores you gave to it. Running the pipeline I highly recommend to take a look at the options that are available since I won't cover everything here. Modes You have two main ways to run the pipeline. You can either just run snakemake --use-conda --directory WORKING_DIR in the root folder containing your experiment and it will run everything without stopping. You can also run each step separately. The main advantage of the second way is that you are able to fine tune your parameters based on the results of fastqc, filtering, mapping quality, etc... I would suggest using the second approach when you work on a new protocol and the first one when you are confident of your parameters. There are seven different modes available and to run one specifically you need to call the mode. Example: To run the qc mode: snakemake --cores 8 qc --use-conda --directory WORKING_DIR You can also run multiple modes at the same time if you want: snakemake --cores 8 qc filter --use-conda --directory WORKING_DIR Single species: meta : Downloads and generates all the subsequent references files and STAR index needed to run the pipeline. You can run this alone if you just want to create the meta-data file before running a new set of data. qc : Creates fastqc reports of your data. filter : Go from sample_R1.fastq.gz to sample_filtered.fastq.gz ready to be mapped to the genome. This step filters out your data for low quality reads and trims adapter you provided in the FILTER section. map : Go from sample_filtered.fastq.gz to the sample_final.bam read to extract the expression data. This maps the data to the genom. extract : Extract the expression data. You'll get a umi and a count expression matrix from your whole experiment. Mixed species Since v 0.4 the pipeline detects mixed experiments on the fly. Simply run snakemake --directory WORKING_DIR --use-conda . The stepwise approach is not available for mixed experiments. Barcode whitelist In protocols such as SCRBseq, the expected barcodes sequences are known. This pipeline also does allow the use of known barcodes instread of a number of expected cells. In order to use this functionnality you just need to add a whitelist barcode file and provide the name of the file in the configuration in the section: FILTER: barcode_whitelist: name_of_your_whitelist_file The file should be in the WORKING_DIR. Run the pipeline as usual. Advanced options If you have some specific adapters that are not present by default in the ones in the templates folder, you can add whatever adapters you want to trim (as many as you need) following the fasta syntax. FILTER: cutadapt: adapters-file: name_of_your_adapter_file.fa Further options --cores N Use this argument to use X amunt of cores available. --notemp Use this to not delete all the temporary files. Without this option, only files between steps are kept. Use this option if you are troobleshooting the pipeline or you want to analyze in between files yourself. --dryrun or -n Use this to check out what is going to run if you run your command. This is nice to check for potential missing files. Folder Structure This is the folder structure you get in the end: /path/to/your/WORKING_DIR/ | -- RAW_DATA/ | -- RESULT_DIR/ | -- -- logs/ | -- -- -- cluster/ | -- -- plots/ | -- -- reports/ | -- -- summary/ | -- -- samples/ | samples.csv | config.yaml | barcodes.csv | adapter.fa | .snakemake/ RAW_DATA/ Contains all your samples as well as the intermediary files RESULT_DIR/logs/ Contains all the logfiles generated by the pipeline RESULT_DIR/logs/cluster Contains all the logfiles generated by the cluster RESULT_DIR/plots/ Contains all the plots generated by the pipeline RESULT_DIR/reports/ Contains all the reports generated by the pipeline RESULT_DIR/summary/ Contains all the files you might use for downstream analysis (contains barcodes selected per sample per species, final umi/counts expression matrix) RESULT_DIR/samples/ Contains all the sample specific files. Bam files, barcodes used, single sample expression files, etc... samples.csv File containing sample details config.yaml File containing pipeline parameters as well as system parameters adapters.fa File containing all the adapters you wish to trim from the raw data. .snakemake/ Folder that contains all the environements created for the run as well as a lot of other things.","title":"Running dropSeqPipe"},{"location":"Running-dropSeqPipe/#example","text":"The pipeline is to be cloned once and then run on any folder containing the configuration files and your raw data. The workingdir folder can contain multiple runs (aka batches) as you can easily add new samples when recieving new data and run the same commands. This will simply run the pipeline on the newly added data and recreate reports as well as plots containing all the samples. Example: You run 2 biological conditions with 2 replicates. This makes up for 4 samples. Assume a simple dropseq protocol with only human cells. 1. You sequence the data and recieve the 8 files (two files per sample) and download the pipeline 2. You run the pipeline with the command: snakemake --use-conda --cores N --directory WORKING_DIR . N being the number of cores available and WORKING_DIR being the folder containing your config.yaml , samples.csv , adapter file and gtf_biotypes.yaml . 3. You see that there is an issue with the protocol and you modify it 4. You create a new set of libraries and sequence them (same 2x2 design) 5. You add the new files in the data folder of WORKING_DIR and edit the samples.csv to add missing samples. 6. You run the pipeline as you did the first time snakemake --use-conda --cores N --directory WORKING_DIR 7. This will run the new samples only and recreate the reports as well as the yield plots. 8. It is now easy to compare the impact of your change in the procotol","title":"Example"},{"location":"Running-dropSeqPipe/#working-dir-folder-preparation","text":"The raw data from the sequencer should be stored in the RAW_DATA folder of WORKING_DIR folder like this: /path/to/your/WORKING_DIR/ | -- RAW_DATA/ | -- -- sample1_R1.fastq.gz | -- -- sample1_R2.fastq.gz | -- -- sample2_R1.fastq.gz | -- -- sample2_R2.fastq.gz | samples.csv | config.yaml | barcodes.csv | adapters.fa Note: In DropSeq or ScrbSeq you expect a paired sequencing. R1 will hold the information of your barcode and UMI, R2 will hold the 3' end of the captured mRNA. Once everything is in place, you can run the pipeline using the normal snakemake commands.","title":"Working dir folder preparation"},{"location":"Running-dropSeqPipe/#running-the-pipeline-tldr-version","text":"For a simple single cell run you only need to run: snakemake --cores N --use-conda --directory WORKING_DIR This will run the whole pipeline and use the X number of cores you gave to it.","title":"Running the pipeline (TLDR version)"},{"location":"Running-dropSeqPipe/#running-the-pipeline","text":"I highly recommend to take a look at the options that are available since I won't cover everything here.","title":"Running the pipeline"},{"location":"Running-dropSeqPipe/#modes","text":"You have two main ways to run the pipeline. You can either just run snakemake --use-conda --directory WORKING_DIR in the root folder containing your experiment and it will run everything without stopping. You can also run each step separately. The main advantage of the second way is that you are able to fine tune your parameters based on the results of fastqc, filtering, mapping quality, etc... I would suggest using the second approach when you work on a new protocol and the first one when you are confident of your parameters. There are seven different modes available and to run one specifically you need to call the mode. Example: To run the qc mode: snakemake --cores 8 qc --use-conda --directory WORKING_DIR You can also run multiple modes at the same time if you want: snakemake --cores 8 qc filter --use-conda --directory WORKING_DIR","title":"Modes"},{"location":"Running-dropSeqPipe/#single-species","text":"meta : Downloads and generates all the subsequent references files and STAR index needed to run the pipeline. You can run this alone if you just want to create the meta-data file before running a new set of data. qc : Creates fastqc reports of your data. filter : Go from sample_R1.fastq.gz to sample_filtered.fastq.gz ready to be mapped to the genome. This step filters out your data for low quality reads and trims adapter you provided in the FILTER section. map : Go from sample_filtered.fastq.gz to the sample_final.bam read to extract the expression data. This maps the data to the genom. extract : Extract the expression data. You'll get a umi and a count expression matrix from your whole experiment.","title":"Single species:"},{"location":"Running-dropSeqPipe/#mixed-species","text":"Since v 0.4 the pipeline detects mixed experiments on the fly. Simply run snakemake --directory WORKING_DIR --use-conda . The stepwise approach is not available for mixed experiments.","title":"Mixed species"},{"location":"Running-dropSeqPipe/#barcode-whitelist","text":"In protocols such as SCRBseq, the expected barcodes sequences are known. This pipeline also does allow the use of known barcodes instread of a number of expected cells. In order to use this functionnality you just need to add a whitelist barcode file and provide the name of the file in the configuration in the section: FILTER: barcode_whitelist: name_of_your_whitelist_file The file should be in the WORKING_DIR. Run the pipeline as usual.","title":"Barcode whitelist"},{"location":"Running-dropSeqPipe/#advanced-options","text":"If you have some specific adapters that are not present by default in the ones in the templates folder, you can add whatever adapters you want to trim (as many as you need) following the fasta syntax. FILTER: cutadapt: adapters-file: name_of_your_adapter_file.fa","title":"Advanced options"},{"location":"Running-dropSeqPipe/#further-options","text":"--cores N Use this argument to use X amunt of cores available. --notemp Use this to not delete all the temporary files. Without this option, only files between steps are kept. Use this option if you are troobleshooting the pipeline or you want to analyze in between files yourself. --dryrun or -n Use this to check out what is going to run if you run your command. This is nice to check for potential missing files.","title":"Further options"},{"location":"Running-dropSeqPipe/#folder-structure","text":"This is the folder structure you get in the end: /path/to/your/WORKING_DIR/ | -- RAW_DATA/ | -- RESULT_DIR/ | -- -- logs/ | -- -- -- cluster/ | -- -- plots/ | -- -- reports/ | -- -- summary/ | -- -- samples/ | samples.csv | config.yaml | barcodes.csv | adapter.fa | .snakemake/ RAW_DATA/ Contains all your samples as well as the intermediary files RESULT_DIR/logs/ Contains all the logfiles generated by the pipeline RESULT_DIR/logs/cluster Contains all the logfiles generated by the cluster RESULT_DIR/plots/ Contains all the plots generated by the pipeline RESULT_DIR/reports/ Contains all the reports generated by the pipeline RESULT_DIR/summary/ Contains all the files you might use for downstream analysis (contains barcodes selected per sample per species, final umi/counts expression matrix) RESULT_DIR/samples/ Contains all the sample specific files. Bam files, barcodes used, single sample expression files, etc... samples.csv File containing sample details config.yaml File containing pipeline parameters as well as system parameters adapters.fa File containing all the adapters you wish to trim from the raw data. .snakemake/ Folder that contains all the environements created for the run as well as a lot of other things.","title":"Folder Structure"}]}